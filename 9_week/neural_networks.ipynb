{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "975982b4",
   "metadata": {},
   "source": [
    "# 8. Neural networks and deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a768ac",
   "metadata": {},
   "source": [
    "This week, we'll learn about nueral nets and build a model for classifying images of clothes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e11094",
   "metadata": {},
   "source": [
    "## 8.1 Fashion classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60afbf3",
   "metadata": {},
   "source": [
    "Dataset\n",
    "\n",
    "- Full https://github.com/alexeygrigorev/clothing-dataset\n",
    "- Small: https://github.com/alexeygrigorev/clothing-dataset-small\n",
    "\n",
    "Links:\n",
    "- https://cs231n.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbabfede-e092-4c6a-a438-8b8a9795e961",
   "metadata": {},
   "source": [
    "## 8.2 TensorFlow and Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0287c8-297b-4adf-b8f0-9bdf0111e83b",
   "metadata": {},
   "source": [
    "- Installing TensorFlow\n",
    "- Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e31fc6-0207-4982-b0de-2587182de539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0005c1-5683-4370-93b7-9bc2103094cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26083f8-5a30-47d7-9493-fbac7de294f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5718ad7-0e15-4f51-9927-6e292c4aa833",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './clothing-dataset-small/train/t-shirt'\n",
    "name = '0add1694-17d0-46ec-a9fc-900da252af41.jpg'\n",
    "fullname = f'{path}/{name}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bce78c-71da-4aa4-b42c-3bbb95648d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "imag = load_img(fullname, target_size=(299, 299))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211041db-1703-44c6-8a21-d0f94da4a63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a52c1-b046-4f7a-8863-c6e80a52b312",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(imag)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02eb58f-f08b-42c1-8d70-d1bf146a243a",
   "metadata": {},
   "source": [
    "## 8.3 Pre-trained convolutional neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb63bfb-0907-4aed-8911-af5fc46b9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from tensorflow.keras.applications.xception import decode_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088a0fa-63d8-4659-987a-87589a72bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Xception(weights='imagenet', input_shape=(299, 299, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f66ac17-95cf-474d-bad7-59c21df951ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73699e79-128e-46e7-9f40-bf86413f4f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d76adb-8d0d-46b5-941f-6a7741ed95f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35fed0e-52bb-48d1-8ed1-2e114f0822c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e1ce66-7d99-4086-877c-de7159b86e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode_predictions(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98293f7-7b7d-4038-a214-55d8d284180b",
   "metadata": {},
   "source": [
    "## 8.4 Convoluational neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d19821-989b-4319-aff1-61ace92e26e5",
   "metadata": {},
   "source": [
    "- Types of layers: convolutional and dense\n",
    "- Convolutional layers and filters\n",
    "- Dense layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd01950-104d-4afb-8e06-38ff502d0b60",
   "metadata": {},
   "source": [
    "## 8.5 Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3498e-c206-42d9-82c2-d29fa85a5fb3",
   "metadata": {},
   "source": [
    "- Reading data with ImageDataGenerator\n",
    "- Train Xception on smaller images (150x150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9b3e0-320b-412d-9cf3-b27757c89bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058c8ba-e993-4909-a66a-2cf0f15afdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a777ba6-a3b7-430d-b4c9-5340e53e404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    './clothing-dataset-small/train', \n",
    "    target_size=(150, 150), \n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80868fc-3160-42f1-8abe-d022fe7048a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668440e8-dbf0-44ae-908d-c598d6e6cd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29518828-45d8-4df6-983e-21b1634c709f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "val_ds = val_gen.flow_from_directory(\n",
    "    './clothing-dataset-small/validation', \n",
    "    target_size=(150, 150), \n",
    "    batch_size=32,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339d98d-bb17-4cfe-8ae1-3b39fb2e266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = Xception(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape=(150,150,3)) \n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "inputs = keras.Input(shape=(150, 150, 3))\n",
    "\n",
    "base = base_model(inputs, training=False)\n",
    "\n",
    "vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
    "\n",
    "outputs = keras.layers.Dense(10)(vectors)\n",
    "\n",
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47239520-fe8e-4135-9c55-26525a117b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ce081-3a54-4951-a5c9-3ab93b23967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=10, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9712d0e-8e5b-4bfa-9f7b-7b5bb4d7d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='train')\n",
    "plt.plot(history.history['val_accuracy'], label='val')\n",
    "plt.xticks(np.arange(10))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c16639d-a81d-4749-9632-79b736b385fe",
   "metadata": {},
   "source": [
    "## 8.6 Adjusting the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced98162-40c0-44af-a452-6928b9fc2a44",
   "metadata": {},
   "source": [
    "- What is the learning rate\n",
    "- Trying different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093b378-4f38-490a-b932-56e85c7ad9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.01, size_inner=100):\n",
    "    base_model = Xception(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape=(150,150,3)) \n",
    "\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    ############################################\n",
    "\n",
    "    inputs = keras.Input(shape=(150, 150, 3))\n",
    "    base = base_model(inputs, training=False)\n",
    "    vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
    "    \n",
    "    inner = keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
    "    \n",
    "    outputs = keras.layers.Dense(10)(inner)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    ############################################\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1313e050-8eb9-4ed5-977f-95c8c0841c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for lr in [0.0001, 0.001, 0.01, 0.1]:\n",
    "    print(lr)\n",
    "    \n",
    "    model=make_model(learning_rate=lr)\n",
    "    history = model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
    "    scores[lr] = history.history\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9f845-7aed-4c2d-ae4f-27eb1566188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_c = scores.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55febbf3-682b-4b11-9f21-33bac84df352",
   "metadata": {},
   "outputs": [],
   "source": [
    "del scores_c[0.1]\n",
    "del scores_c[0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f27276-fab0-4b34-8530-38a5a32358fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr, hist in scores_c.items():\n",
    "    plt.plot(hist['val_accuracy'], label=lr)\n",
    "    \n",
    "plt.xticks(np.arange(10))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257ff59-850d-4599-a6a1-3b2af8d38dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8097d4-58ac-4e32-b886-8fbdd02c8e2a",
   "metadata": {},
   "source": [
    "## 8.7 Checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4e819b-7297-4c84-9eef-e45a0a4d7d32",
   "metadata": {},
   "source": [
    "- Saving the best model only\n",
    "- Training a model with callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cafa526-462a-4f19-8654-6e687ca9d2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    " model.save_weights('model_v1.h5', save_format='h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22284963-c03c-4f1f-81e3-f7d9a99cdea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'xception_v1_{epoch:02d}_{val_accuracy:.3f}.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1470e4d-b041-4232-820f-07a04ca26ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "model=make_model(learning_rate=lr)\n",
    "\n",
    "history = model.fit(train_ds, \n",
    "                    epochs=10, \n",
    "                    validation_data=val_ds,\n",
    "                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b73d83-c8a4-42b3-9245-28e1bffb96bc",
   "metadata": {},
   "source": [
    "## 8.8 Adding more layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a582b8ca-4cf5-412e-8c1b-55dbda550148",
   "metadata": {},
   "source": [
    "- Adding one inner dense layer\n",
    "- Experimenting with different sizes of inner layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f520412-4640-4d1a-b3b9-b69fd66c16dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for size in [10, 100, 1000]:\n",
    "    print(size)\n",
    "    \n",
    "    model=make_model(learning_rate=learning_rate, size_inner=size)\n",
    "    history = model.fit(train_ds, epochs=10, validation_data=val_ds)\n",
    "    scores[size] = history.history\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3757993-d1d6-44b8-aa27-4a8ca6c4bc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "for size, hist in scores.items():\n",
    "    plt.plot(hist['val_accuracy'], label=('val=%s' % size))\n",
    "    \n",
    "plt.xticks(np.arange(10))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff539ae-785d-497f-b173-40dbb78e159a",
   "metadata": {},
   "source": [
    "## 8.9 Regularization and dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1319360-a1cf-4ad0-bf25-ad0975b23fe5",
   "metadata": {},
   "source": [
    "- Regularizing by freezing a part of the network\n",
    "- Adding dropout to our model\n",
    "- Experimenting with different values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afae7a99-622c-4959-b616-fb12bffe3981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(learning_rate=0.01, size_inner=100, droprate=0.50):\n",
    "    base_model = Xception(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape=(150,150,3)) \n",
    "\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    ############################################\n",
    "\n",
    "    inputs = keras.Input(shape=(150, 150, 3))\n",
    "    base = base_model(inputs, training=False)\n",
    "    vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
    "    \n",
    "    inner = keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
    "    drop = keras.layers.Dropout(droprate)(inner)\n",
    "    \n",
    "    outputs = keras.layers.Dense(10)(drop)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    ############################################\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c1064c-cfcf-4028-9aae-380fdf2bc357",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "size = 100\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for droprate in [0.0, 0.2, 0.5, 0.8]:\n",
    "    print(droprate)\n",
    "    \n",
    "    model=make_model(learning_rate=learning_rate, \n",
    "                     size_inner=size,\n",
    "                     droprate=droprate)\n",
    "    history = model.fit(train_ds, epochs=30, validation_data=val_ds)\n",
    "    scores[droprate] = history.history\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d67b61-d178-41be-8707-7daf29d25435",
   "metadata": {},
   "outputs": [],
   "source": [
    "for droprate, hist in scores.items():\n",
    "    plt.plot(hist['val_accuracy'], label=('val=%s' % droprate))\n",
    "    \n",
    "plt.ylim(0.78,0.86)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead6ea3-410f-4f40-b339-668669ed45f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "his = scores[0.0]\n",
    "plt.plot(his['val_accuracy'], label=0.0)\n",
    "\n",
    "his = scores[0.2]\n",
    "plt.plot(his['val_accuracy'], label=0.2)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14a322-7dd9-481e-95af-a0fbb7d0ce1b",
   "metadata": {},
   "source": [
    "## 8.10 Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863dc6d-472b-43e9-a21c-4a7fd380c5e8",
   "metadata": {},
   "source": [
    "- Different data augmentations\n",
    "- Training a model with augmentations\n",
    "- How to select data augmentations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd7f6dc-bf42-4f36-b698-64de68229d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range=10,\n",
    "    zoom_range=0.1,\n",
    "    vertical_flip=True \n",
    ")\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    './clothing-dataset-small/train',\n",
    "    target_size=(150,150),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "val_ds = val_gen.flow_from_directory(\n",
    "    './clothing-dataset-small/validation',\n",
    "    target_size=(150,150),\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd86bd5-a753-4cde-a592-b09a4d35c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate= 0.001\n",
    "size = 100\n",
    "droprate = 0.2\n",
    "\n",
    "model = make_model(\n",
    "    learning_rate=learning_rate,\n",
    "    size_inner=size,\n",
    "    droprate=droprate)\n",
    "\n",
    "history = model.fit(train_ds, epochs=50, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750470a2-1939-4cdb-af5c-beef45e895ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "his = history.history\n",
    "plt.plot(his['val_accuracy'], label='val')\n",
    "plt.plot(his['accuracy'], label='train')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0568b5d8-c227-417e-ad31-a52591be5d12",
   "metadata": {},
   "source": [
    "## 8.11 Training a larger model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927bf6d8-8674-4dcb-8848-1d519e81df50",
   "metadata": {},
   "source": [
    "- Train a 299x299 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8a976b-5db7-438e-8df5-225bbc630418",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(input_size=150, learning_rate=0.01, size_inner=100, droprate=0.50):\n",
    "    base_model = Xception(weights='imagenet', \n",
    "                      include_top=False, \n",
    "                      input_shape=(input_size,input_size,3)) \n",
    "\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    ############################################\n",
    "\n",
    "    inputs = keras.Input(shape=(input_size, input_size, 3))\n",
    "    base = base_model(inputs, training=False)\n",
    "    vectors = keras.layers.GlobalAveragePooling2D()(base)\n",
    "    \n",
    "    inner = keras.layers.Dense(size_inner, activation='relu')(vectors)\n",
    "    drop = keras.layers.Dropout(droprate)(inner)\n",
    "    \n",
    "    outputs = keras.layers.Dense(10)(drop)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    ############################################\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    loss = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss=loss, \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134417cb-075b-4f57-86a7-b99d57ab8369",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b888c2-8103-4130-b580-71e392ff3cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    shear_range=10,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True \n",
    ")\n",
    "\n",
    "train_ds = train_gen.flow_from_directory(\n",
    "    './clothing-dataset-small/train',\n",
    "    target_size=(input_size,input_size),\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "val_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "val_ds = val_gen.flow_from_directory(\n",
    "    './clothing-dataset-small/validation',\n",
    "    target_size=(input_size,input_size),\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c18c00-ca7a-472a-8ed2-b36ca5007ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "    'xception_v4_{epoch:02d}_{val_accuracy:.3f}.h5',\n",
    "    save_best_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88af0ad1-f6f9-407e-a1de-d782e7ddd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate= 0.001\n",
    "size = 100\n",
    "droprate = 0.2\n",
    "\n",
    "model = make_model(\n",
    "    learning_rate=learning_rate,\n",
    "    size_inner=size,\n",
    "    droprate=droprate)\n",
    "\n",
    "history = model.fit(train_ds, epochs=50, validation_data=val_ds, callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c950299-1eaa-4e95-9513-e52680603926",
   "metadata": {},
   "source": [
    "## 8.12 Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75042517-5375-4b21-86d3-e08e5048a99a",
   "metadata": {},
   "source": [
    "- Loading the model\n",
    "- Evaluating the model\n",
    "- Getting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "677ff749-6224-4890-b38a-e74fb06dbd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80a090ab-771d-4b0d-bc4d-65e810b0831e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('xception_v4_06_0.889.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "381183ac-9163-4690-930b-f9f5290fd343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "from tensorflow.keras.applications.xception import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d04917f9-617f-437d-aca0-ae747321fdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 372 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "test_gen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_ds = test_gen.flow_from_directory(\n",
    "    './clothing-dataset-small/test',\n",
    "    target_size=(299,299),\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4652c73f-8a46-4595-adef-da8afb3bece3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 8s 604ms/step - loss: 0.2840 - accuracy: 0.9059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2839863896369934, 0.9059139490127563]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8d8ca96-07b9-4f27-9028-64fabfc2d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'clothing-dataset-small/test/pants/c8d21106-bbdb-4e8d-83e4-bf3d14e54c16.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c213867-2721-491f-81d4-157e3d29abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = load_img(path, target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "76d13646-aa53-4b1c-b13a-b1fa78eeed92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150, 150, 3)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(img)\n",
    "X = np.array([x])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c711fcdf-7bb7-4599-ac16-5a0ece2c9c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_input(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5663c9d5-cafd-424a-929f-27470b88730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 103ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7652809b-9f97-4675-90a7-85ca643f3e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['dress', 'hat', 'longsleeve', 'outwear', 'pants', 'shirt',\n",
    "           'shoes', 'shorts', 'skirt', 't-shirt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "391b8322-92b1-463d-9d02-92d992e18b35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dress': -9.209121,\n",
       " 'hat': -5.613119,\n",
       " 'longsleeve': -3.1857297,\n",
       " 'outwear': 0.58136064,\n",
       " 'pants': 17.356606,\n",
       " 'shirt': -2.082063,\n",
       " 'shoes': -4.9132404,\n",
       " 'shorts': 3.6366785,\n",
       " 'skirt': -13.593643,\n",
       " 't-shirt': -8.056492}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(classes, pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0022eb-ca6e-4857-b1bf-a0941832b9a6",
   "metadata": {},
   "source": [
    "## 8.13 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1dabba-79ef-4fb2-89de-0b50a128dd66",
   "metadata": {},
   "source": [
    "- We can use pre-trained models for general image classification.\n",
    "- Convolutional layers let us turn an image into a vector.\n",
    "- Dense layers use the vector to make predictions.\n",
    "- Instead of training a model from scratch, we can use transfer learning and re-use already trained convolutional layers.\n",
    "- First, train a small model (150x150) before training a big one (299x299)\n",
    "- Learning rate how fast the model trains. Fast learners aren't always the best ones.\n",
    "- We can save the best model using callbacks and checkpointing.\n",
    "- To avoid overfitting, use dropout and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0436bd-d6ce-4ca3-972c-781987100ab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
